mu = input[1] ## do this for optim function
sig1 = input[2]
n=length(X)
Cov = getCovRE2(sig1,R)
# term1 = -n*log(2*pi)/2 + -log(det(Cov))/2
# term2 =  -1/2*t(X-rep(mu,n))%*%solve(Cov)%*%(X-rep(mu,n))
# return (-1*(term1 + term2)) # return the log version. NEGATIVE TO USE OPTIM FUNC.
k = dmvnorm( x=X, mean=rep(mu, n), sigma=Cov )
return ( -1* log(k) )
}
getCovRE2 = function ( sig1,R ) {
Indentity = diag(1,ncol(R))
return ( sig1 * Indentity + R ) ## const U + diag
}
#### functions : FE
getLikelihoodFE = function (input,X,R) { ## get the log likelihood of the data (either L0 or L1)
mu = input[1] ## do this for optim function
Cov = R
n=length(X)
k = dmvnorm( x=X, mean=rep(mu, n), sigma=Cov )
return ( -1* log(k) )
}
## call libraries
library("mvtnorm")
#library("matrixcalc")
#### functions: test input and output
testinput = function(filename){
if (file.exists(filename) == F ){
print ("There is noinput with this name.")
q()
return(filename)
}
return(filename)
}
testoutput = function(filename){
if (file.exists(filename)){
print ("There is a file with the same name.")
#q()
return(filename)
}
return(filename)
}
nostuck = 0
best = "try"
class(best)="try-error"
while (class(best)=="try-error"){
best = try ( constrOptim( runif(2,.001,.2), getLikelihood, X=beta,n=n, U=U, R=R, ui=ui, ci=ci, method = c("Nelder-Mead"), control = list(maxit=1000) ) , silent = F)
nostuck = nostuck+1
if (nostuck>10){
print("ERROR")
print(snp)
print(beta)
print(sd_beta)
print(U)
print(R)
break
}
}
alt_val = -1* best$value ## alternative , -1 to reverse the scale
this_lr = 2* (alt_val-nul_val) # likelihood ratio
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
stat2_cdf
FEs
(this_lr - FEs[1000])
max(0,this_lr - FEs[1000])
20 * max(0,this_lr - FEs[1000])
floor(20 * max(0,this_lr - FEs[1000]))
p.RE2C <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
p.RE2C
FEs
REg_ext <- function(this_lr,stat2_cdf,n){
extra<-this_lr-50
modFEs<-seq(extra+0.25,this_lr,0.25)
modFEprobs<-cal_FEprobs(modFEs)
theP=(((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,200),(rep(50,200)-FEs_ext)),1,max)))+1])%*%modFEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(this_lr, 1, lower.tail=F))
return(theP)
}
sumstats = rep(1.2,7)
nonNA = which ( ! is.na(sumstats) ) ## valid Zscores
if (length(nonNA)==0){
next
}
beta = as.numeric(sumstats[nonNA])
n = length(beta)
sd_beta = rep(1,n)
D = diag(sd_beta^2,n) # diag sampling errors
R = diag(sd_beta) %*% reCor[nonNA,nonNA] %*% diag(sd_beta) # variance-covariance matrix of random errors
U = genCor[nonNA,nonNA]
## null values
nul_val = log ( dnorm( beta, 0, sd=sd_beta ) )
nul_val [ nul_val == -Inf ] = min ( nul_val[nul_val!=-Inf] )
nul_val = sum(nul_val) ## avoid NA in the null
## do optim
ui = matrix( c(1,0,-1,0,0,1), ncol= 2, byrow=T ) ## constraint
ci = matrix( c(-200,-200, 0 ), ncol = 1 ) ## Cue :
nostuck = 0
best = "try"
class(best)="try-error"
while (class(best)=="try-error"){
best = try ( constrOptim( runif(2,.001,.2), getLikelihood, X=beta,n=n, U=U, R=R, ui=ui, ci=ci, method = c("Nelder-Mead"), control = list(maxit=1000) ) , silent = F)
nostuck = nostuck+1
if (nostuck>10){
print("ERROR")
print(snp)
print(beta)
print(sd_beta)
print(U)
print(R)
break
}
}
if (nostuck>10){ ## fail to optimize
obs_pval = alt_val = muc = varc = 9999
} else{
alt_val = -1* best$value ## alternative , -1 to reverse the scale
this_lr = 2* (alt_val-nul_val) # likelihood ratio
if ( n == 1 ){obs_pval = pchisq(this_lr,df=1,ncp=0,lower.tail=F)
} else {
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
}
muc = best$par[1]
varc = best$par[2]
}
obs_pval
this_lr
alt_val
nul_val
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
n-1
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
correction.list
stat2_cdf
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
obs_pval
sumstats = rep(3.2,7)
nonNA = which ( ! is.na(sumstats) ) ## valid Zscores
if (length(nonNA)==0){
next
}
beta = as.numeric(sumstats[nonNA])
n = length(beta)
sd_beta = rep(1,n)
D = diag(sd_beta^2,n) # diag sampling errors
R = diag(sd_beta) %*% reCor[nonNA,nonNA] %*% diag(sd_beta) # variance-covariance matrix of random errors
U = genCor[nonNA,nonNA]
### -------------------------------------------------------
### fit the likelihood
### -------------------------------------------------------
## null values
nul_val = log ( dnorm( beta, 0, sd=sd_beta ) )
nul_val [ nul_val == -Inf ] = min ( nul_val[nul_val!=-Inf] )
nul_val = sum(nul_val) ## avoid NA in the null
## do optim
ui = matrix( c(1,0,-1,0,0,1), ncol= 2, byrow=T ) ## constraint
ci = matrix( c(-200,-200, 0 ), ncol = 1 ) ## Cue :
nostuck = 0
best = "try"
class(best)="try-error"
while (class(best)=="try-error"){
best = try ( constrOptim( runif(2,.001,.2), getLikelihood, X=beta,n=n, U=U, R=R, ui=ui, ci=ci, method = c("Nelder-Mead"), control = list(maxit=1000) ) , silent = F)
nostuck = nostuck+1
if (nostuck>10){
print("ERROR")
print(snp)
print(beta)
print(sd_beta)
print(U)
print(R)
break
}
}
if (nostuck>10){ ## fail to optimize
obs_pval = alt_val = muc = varc = 9999
} else{
alt_val = -1* best$value ## alternative , -1 to reverse the scale
this_lr = 2* (alt_val-nul_val) # likelihood ratio
if ( n == 1 ){obs_pval = pchisq(this_lr,df=1,ncp=0,lower.tail=F)
} else {
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
}
muc = best$par[1]
varc = best$par[2]
}
writeout = c(snp, obs_pval, nul_val, alt_val, muc, varc )
## fit RE2 ### -------------------------------------------------------
nostuck = 0
best = "try"
class(best)="try-error"
while (class(best)=="try-error"){
best = try ( constrOptim( runif(2,.001,.2), getLikelihoodRE2, X=beta, n=n, R=R, ui=ui, ci=ci, method = c("Nelder-Mead"), control = list(maxit=1000) ) , silent=T)
nostuck = nostuck+1
if (nostuck>10){
break
}
}
if (nostuck>10){ ## fail to optimize
obs_pvalR2 = alt_val = muc = varc = 9999
} else{
alt_val = -1* best$value ## alternative , -1 to reverse the scale
this_lr = 2* (alt_val-nul_val) # likelihood ratio
if ( n == 1 ){obs_pval = pchisq(this_lr,df=1,ncp=0,lower.tail=F)
} else {
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
}
muc = best$par[1]
varc = best$par[2]
}
obs_pval
nonNA = which ( ! is.na(sumstats) ) ## valid Zscores
if (length(nonNA)==0){
next
}
beta = as.numeric(sumstats[nonNA])
n = length(beta)
sd_beta = rep(1,n)
D = diag(sd_beta^2,n) # diag sampling errors
R = diag(sd_beta) %*% reCor[nonNA,nonNA] %*% diag(sd_beta) # variance-covariance matrix of random errors
U = genCor[nonNA,nonNA]
### -------------------------------------------------------
### fit the likelihood
### -------------------------------------------------------
## null values
nul_val = log ( dnorm( beta, 0, sd=sd_beta ) )
nul_val [ nul_val == -Inf ] = min ( nul_val[nul_val!=-Inf] )
nul_val = sum(nul_val) ## avoid NA in the null
## do optim
ui = matrix( c(1,0,-1,0,0,1), ncol= 2, byrow=T ) ## constraint
ci = matrix( c(-200,-200, 0 ), ncol = 1 ) ## Cue :
nostuck = 0
best = "try"
class(best)="try-error"
while (class(best)=="try-error"){
best = try ( constrOptim( runif(2,.001,.2), getLikelihood, X=beta,n=n, U=U, R=R, ui=ui, ci=ci, method = c("Nelder-Mead"), control = list(maxit=1000) ) , silent = F)
nostuck = nostuck+1
if (nostuck>10){
print("ERROR")
print(snp)
print(beta)
print(sd_beta)
print(U)
print(R)
break
}
}
if (nostuck>10){ ## fail to optimize
obs_pval = alt_val = muc = varc = 9999
} else{
alt_val = -1* best$value ## alternative , -1 to reverse the scale
this_lr = 2* (alt_val-nul_val) # likelihood ratio
if ( n == 1 ){obs_pval = pchisq(this_lr,df=1,ncp=0,lower.tail=F)
} else {
## correction factors
RE2Cor <- RE2Cor.list[[min(n-1,6)]]
tau2prob_cor <- tau2prob_corlist[[min(n-1,6)]]
## run correction.function(cor)
correction.list <- correction.function(reCor[nonNA,nonNA])
RE2C_corr <- correction.list[[1]];
cur_tau2.zero.prob <- correction.list[[2]];
stat2_cdf <- as.numeric(RE2C_table[n-1,]) * RE2C_corr
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
}
muc = best$par[1]
varc = best$par[2]
}
obs_pval
this_lr
pchisq(71,1,0,lower.tail = F)
pchisq(71.84,1,0,lower.tail = F)
chisq = 24
pchisq(chisq,1,0,lower.tail = F)
this_lr = 24
if(this_lr > 50){
obs_pval <- REg_ext(this_lr,stat2_cdf=stat2_cdf,n)
} else {  obs_pval <- ((1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20*(apply(cbind(rep(0,1000),(rep(this_lr,1000)-FEs)),1,max)))+1])%*%FEprobs + (1-cur_tau2.zero.prob[n-1])*stat2_cdf[floor(20 * max(0,this_lr - FEs[1000])) + 1]*pchisq(50, 1, lower.tail=F)
}
obs_pval
obs_pval
pchisq(chisq,1,0,lower.tail = F)
## Load data
## The table is to correct bias made by large sample number assumption for S_het
raw.tau2.zero.prob <<- c(0.843, 0.777, 0.739, 0.713, 0.6943, 0.6794, 0.6674, 0.658, 0.6498, 0.6432, 0.6368, 0.6315, 0.6268, 0.6223, 0.6184, 0.6148, 0.6118, 0.6088, 0.6062, 0.6036, 0.6011, 0.5987, 0.5968, 0.595, 0.5932, 0.5915, 0.5898, 0.5884, 0.5867, 0.585, 0.584, 0.5827, 0.5815, 0.5807, 0.579, 0.5782, 0.5774, 0.5763, 0.5751, 0.5742, 0.5736, 0.5729, 0.5717, 0.571, 0.5703, 0.5697, 0.569, 0.5679, 0.5675)## From nstudy 1 to nstudy 50 (50 entries)
load(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/tau2prob_cor.rdata")
RE2Corf <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2Cor.RData"
load(RE2Corf)
## RE2C table has the cumulative densities of a thousand points with 0.05 intervals for study number from 2 to 50
RE2C_tablef <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2C_table.txt"
RE2C_table <- as.matrix(read.table(RE2C_tablef, header=T,check.names=F))
## FEtlow table has the t.low values of a thousand points with 0.05 intervals for study number from 2 to 50
FEtlow_tablef <- "/Users/cuelee/OneDrive/softwares/Cue/RE2C/RE2.5code/tables/FEtlow_table.txt"
FEtlow_table <- as.matrix(read.table(FEtlow_tablef, header=T,check.names=F))
## FEs are intervals of each FEtlow_table[i,]
FEs <- as.numeric(colnames(FEtlow_table))
### RE2.5 need percentile of each 1000 bins
cal_FEprobs<- function(FEs){
pchisqs<-pchisq(FEs,df=1,ncp=0,lower.tail=F)
inst=1
FEprobs=NULL
for (i in 1:length(FEs)){
FEprobs <- c(FEprobs,inst-pchisqs[i])
inst=pchisqs[i]
}
return(FEprobs)
}
FEprobs <- cal_FEprobs(FEs)
FEs_ext <- seq(0.25,50,0.25)
remove(RE2C_tablef)
remove(FEtlow_tablef)
remove(RE2Corf)
## Load Function
## RE2C uses tabulated matrices to correct stat2_cdf and FEt.lows
correction.function <- function(correlationMatrix){
mean_cor <- mean(correlationMatrix[lower.tri(correlationMatrix)])
int_corr <- floor(mean_cor*10)
float_corr <- (mean_cor*10) - int_corr
corr_coeff <- int_corr+1
aset <- c(corr_coeff,corr_coeff+1)
#diff(matrix(x)) If x is a matrix then the difference operations are carried out on each column separately.
RE2C_corr<- RE2Cor[corr_coeff,] + diff(RE2Cor[aset,])*float_corr
tau2.zero.prob_corr <- tau2prob_cor[corr_coeff]+ diff(tau2prob_cor[aset])*float_corr
tau2.zero.prob <- raw.tau2.zero.prob*tau2.zero.prob_corr
return(list(RE2C_corr,tau2.zero.prob))
}
save.image(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE_general.Rdata")
load("/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE_general.Rdata")
stat2_cdf
## Load data
## The table is to correct bias made by large sample number assumption for S_het
raw.tau2.zero.prob <<- c(0.843, 0.777, 0.739, 0.713, 0.6943, 0.6794, 0.6674, 0.658, 0.6498, 0.6432, 0.6368, 0.6315, 0.6268, 0.6223, 0.6184, 0.6148, 0.6118, 0.6088, 0.6062, 0.6036, 0.6011, 0.5987, 0.5968, 0.595, 0.5932, 0.5915, 0.5898, 0.5884, 0.5867, 0.585, 0.584, 0.5827, 0.5815, 0.5807, 0.579, 0.5782, 0.5774, 0.5763, 0.5751, 0.5742, 0.5736, 0.5729, 0.5717, 0.571, 0.5703, 0.5697, 0.569, 0.5679, 0.5675)## From nstudy 1 to nstudy 50 (50 entries)
load(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/tau2prob_cor.rdata")
RE2Corf <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2Cor.RData"
load(RE2Corf)
## RE2C table has the cumulative densities of a thousand points with 0.05 intervals for study number from 2 to 50
RE2C_tablef <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2C_table.txt"
RE2C_table <- as.matrix(read.table(RE2C_tablef, header=T,check.names=F))
## FEtlow table has the t.low values of a thousand points with 0.05 intervals for study number from 2 to 50
FEtlow_tablef <- "/Users/cuelee/OneDrive/softwares/Cue/RE2C/RE2.5code/tables/FEtlow_table.txt"
FEtlow_table <- as.matrix(read.table(FEtlow_tablef, header=T,check.names=F))
## FEs are intervals of each FEtlow_table[i,]
FEs <- as.numeric(colnames(FEtlow_table))
### RE2.5 need percentile of each 1000 bins
cal_FEprobs<- function(FEs){
pchisqs<-pchisq(FEs,df=1,ncp=0,lower.tail=F)
inst=1
FEprobs=NULL
for (i in 1:length(FEs)){
FEprobs <- c(FEprobs,inst-pchisqs[i])
inst=pchisqs[i]
}
return(FEprobs)
}
FEprobs <- cal_FEprobs(FEs)
FEs_ext <- seq(0.25,50,0.25)
remove(RE2C_tablef)
remove(FEtlow_tablef)
remove(RE2Corf)
## Load Function
## RE2C uses tabulated matrices to correct stat2_cdf and FEt.lows
correction.function <- function(correlationMatrix){
mean_cor <- mat(mean(correlationMatrix[lower.tri(correlationMatrix)]),0)
int_corr <- floor(mean_cor*10)
float_corr <- (mean_cor*10) - int_corr
corr_coeff <- int_corr+1
aset <- c(corr_coeff,corr_coeff+1)
#diff(matrix(x)) If x is a matrix then the difference operations are carried out on each column separately.
RE2C_corr<- RE2Cor[corr_coeff,] + diff(RE2Cor[aset,])*float_corr
tau2.zero.prob_corr <- tau2prob_cor[corr_coeff]+ diff(tau2prob_cor[aset])*float_corr
tau2.zero.prob <- raw.tau2.zero.prob*tau2.zero.prob_corr
return(list(RE2C_corr,tau2.zero.prob))
}
save.image(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE_general.Rdata")
## Load data
## The table is to correct bias made by large sample number assumption for S_het
raw.tau2.zero.prob <<- c(0.843, 0.777, 0.739, 0.713, 0.6943, 0.6794, 0.6674, 0.658, 0.6498, 0.6432, 0.6368, 0.6315, 0.6268, 0.6223, 0.6184, 0.6148, 0.6118, 0.6088, 0.6062, 0.6036, 0.6011, 0.5987, 0.5968, 0.595, 0.5932, 0.5915, 0.5898, 0.5884, 0.5867, 0.585, 0.584, 0.5827, 0.5815, 0.5807, 0.579, 0.5782, 0.5774, 0.5763, 0.5751, 0.5742, 0.5736, 0.5729, 0.5717, 0.571, 0.5703, 0.5697, 0.569, 0.5679, 0.5675)## From nstudy 1 to nstudy 50 (50 entries)
load(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/tau2prob_cor.rdata")
RE2Corf <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2Cor.RData"
load(RE2Corf)
## RE2C table has the cumulative densities of a thousand points with 0.05 intervals for study number from 2 to 50
RE2C_tablef <- "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE2C_table.txt"
RE2C_table <- as.matrix(read.table(RE2C_tablef, header=T,check.names=F))
## FEtlow table has the t.low values of a thousand points with 0.05 intervals for study number from 2 to 50
FEtlow_tablef <- "/Users/cuelee/OneDrive/softwares/Cue/RE2C/RE2.5code/tables/FEtlow_table.txt"
FEtlow_table <- as.matrix(read.table(FEtlow_tablef, header=T,check.names=F))
## FEs are intervals of each FEtlow_table[i,]
FEs <- as.numeric(colnames(FEtlow_table))
### RE2.5 need percentile of each 1000 bins
cal_FEprobs<- function(FEs){
pchisqs<-pchisq(FEs,df=1,ncp=0,lower.tail=F)
inst=1
FEprobs=NULL
for (i in 1:length(FEs)){
FEprobs <- c(FEprobs,inst-pchisqs[i])
inst=pchisqs[i]
}
return(FEprobs)
}
FEprobs <- cal_FEprobs(FEs)
FEs_ext <- seq(0.25,50,0.25)
remove(RE2C_tablef)
remove(FEtlow_tablef)
remove(RE2Corf)
## Load Function
## RE2C uses tabulated matrices to correct stat2_cdf and FEt.lows
correction.function <- function(correlationMatrix){
mean_cor <- max(mean(correlationMatrix[lower.tri(correlationMatrix)]),0)
int_corr <- floor(mean_cor*10)
float_corr <- (mean_cor*10) - int_corr
corr_coeff <- int_corr+1
aset <- c(corr_coeff,corr_coeff+1)
#diff(matrix(x)) If x is a matrix then the difference operations are carried out on each column separately.
RE2C_corr<- RE2Cor[corr_coeff,] + diff(RE2Cor[aset,])*float_corr
tau2.zero.prob_corr <- tau2prob_cor[corr_coeff]+ diff(tau2prob_cor[aset])*float_corr
tau2.zero.prob <- raw.tau2.zero.prob*tau2.zero.prob_corr
return(list(RE2C_corr,tau2.zero.prob))
}
save.image(file = "/Users/cuelee/Dropbox/Bogdan/RE3_code/code/data/RE_general.Rdata")
#data = read.table("/Users/cuelee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/output.txt",header =T)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/REgeneral.pdf")
hist(data[,2],xlab = "REgeneral_nullp")
dev.off()
colnames(data)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/RE2C.pdf")
hist(data[,2],xlab = "RE2C.pdf")
dev.off()
nrow(data)
data = read.table("/Users/cuelee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/output.txt",header =T)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/REgeneral.pdf")
hist(data[,2],xlab = "REgeneral_nullp")
dev.off()
colnames(data)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/RE2C.pdf")
hist(data[,2],xlab = "RE2C.pdf")
dev.off()
nrow(data)
data = read.table("/Users/cuelee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/output.txt",header =T)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/REgeneral.pdf")
hist(data[,2],xlab = "REgeneral_nullp")
dev.off()
colnames(data)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/RE2C.pdf")
hist(data[,2],xlab = "RE2C.pdf")
dev.off()
nrow(data)
data = read.table("/Users/cuelee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/output.txt",header =T)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/REgeneral.pdf")
hist(data[,2],xlab = "REgeneral_nullp")
dev.off()
colnames(data)
pdf(file = "/Users/CueLee/Dropbox/Bogdan/Cue_Analysis/mainAnalysis/12_testnull/RE2C.pdf")
hist(data[,2],xlab = "RE2C.pdf")
dev.off()
